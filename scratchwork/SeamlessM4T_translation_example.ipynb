{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YBW7_1ASJWsn"
      },
      "outputs": [],
      "source": [
        "input_txt = 'Bonjour, mon chien est mignon.'\n",
        "target_lang = 'eng'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfcv0BJzHpge"
      },
      "source": [
        "# LID (language identification)\n",
        "Using https://huggingface.co/facebook/fasttext-language-identification  \n",
        "this model is CPU only but is pretty fast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8PudWdZHoQP",
        "outputId": "948ddc2c-384c-4a89-b479-40078158a0b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "import fasttext\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# load model\n",
        "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
        "model = fasttext.load_model(model_path)\n",
        "\n",
        "# predict, outputs:((prediction, probablity))\n",
        "lid_prediction = model.predict(input_txt)\n",
        "\n",
        "# extract lang code from prediction\n",
        "lid = lid_prediction[0][0]\n",
        "detected_source_lang = lid[lid.rfind(\"__\")+2:lid.rfind(\"_\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bTDfpJ4WKQu0",
        "outputId": "c20ade91-3fe5-43f8-9010-679acc6f237e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'fra'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detected_source_lang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg1eRzRnIGuk"
      },
      "source": [
        "# Txt translation\n",
        "using SeamlessM4T Large (https://huggingface.co/facebook/hf-seamless-m4t-large) \"seamless\" but with no LID!! raah!    \n",
        "Max word count input: 4096. May need to adjust smaller for good resutls; plz test  \n",
        "Can use CPU or GPU; below uses GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCupYyaYAN0x",
        "outputId": "4415b1e2-2c6a-4e5b-d386-e7f4da74caf2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, SeamlessM4TForTextToText\n",
        "\n",
        "# cuda or cpu\n",
        "device = 'cuda'\n",
        "\n",
        "# load model & tokenizer\n",
        "tokenizer = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-large\", device_map = device, use_fast = False)\n",
        "model = SeamlessM4TForTextToText.from_pretrained(\"facebook/hf-seamless-m4t-large\", device_map = device)\n",
        "\n",
        "# txt to txt translation (source language required)\n",
        "input_tokens = tokenizer(text = input_txt, src_lang=detected_source_lang, return_tensors=\"pt\").to(torch.device(device))\n",
        "output_tokens = model.generate(**input_tokens, tgt_lang=target_lang)\n",
        "translated_text = tokenizer.decode(output_tokens[0].tolist(), skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aWMGjgmvAWNj",
        "outputId": "eb007dd2-f66f-4c03-8ce8-f896fa8c6956"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hi, my dog is cute.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translated_text"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
